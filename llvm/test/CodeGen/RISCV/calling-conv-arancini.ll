; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+d,+f,+v -verify-machineinstrs < %s \
; RUN:   | FileCheck %s

define arancini i64 @func0(ptr %0) {
; CHECK-LABEL: func0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s1, 0(sp) # 8-byte Folded Spill
; CHECK-NEXT:    .cfi_offset ra, -8
; CHECK-NEXT:    .cfi_offset s1, -16
; CHECK-NEXT:    li t1, 1
; CHECK-NEXT:    li t2, 2
; CHECK-NEXT:    li s1, 3
; CHECK-NEXT:    call func1@plt
; CHECK-NEXT:    mv t0, t2
; CHECK-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s1, 0(sp) # 8-byte Folded Reload
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %2 = call arancini { ptr, i64, i64, i64 } @func1(ptr %0, i64 1, i64 2, i64 3)
  %3 = extractvalue { ptr, i64, i64, i64 } %2, 2
  ret i64 %3
}

define arancini { ptr, i64, i64, i64 } @func1(ptr %0, i64 %1, i64 %2, i64 %3) {
; CHECK-LABEL: func1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s1, 0(sp) # 8-byte Folded Spill
; CHECK-NEXT:    .cfi_offset ra, -8
; CHECK-NEXT:    .cfi_offset s1, -16
; CHECK-NEXT:    call func2@plt
; CHECK-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s1, 0(sp) # 8-byte Folded Reload
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %5 = call arancini { ptr, i64, i64, i64 } @func2(ptr %0, i64 %1, i64 %2, i64 %3)
  ret { ptr, i64, i64, i64 } %5
}

define arancini { ptr, i64, i64, i64 } @func2(ptr %0, i64 %1, i64 %2, i64 %3) {
; CHECK-LABEL: func2:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    sd s1, 8(sp) # 8-byte Folded Spill
; CHECK-NEXT:    .cfi_offset s1, -8
; CHECK-NEXT:    addi t1, t1, 1
; CHECK-NEXT:    addi t2, t2, 1
; CHECK-NEXT:    addi s1, s1, 1
; CHECK-NEXT:    ld s1, 8(sp) # 8-byte Folded Reload
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %5 = add i64 %1, 1
  %6 = add i64 %2, 1
  %7 = add i64 %3, 1

  %ret_val = insertvalue { ptr, i64, i64, i64 } undef, ptr %0, 0
  %ret_val2 = insertvalue { ptr, i64, i64, i64 } %ret_val, i64 %5, 1
  %ret_val3 = insertvalue { ptr, i64, i64, i64 } %ret_val2, i64 %6, 2
  %ret_val4 = insertvalue { ptr, i64, i64, i64 } %ret_val3, i64 %7, 3

  ret { ptr, i64, i64, i64 } %ret_val4
}

define arancini { ptr, i64, i64, i128 } @func3(ptr %0, i64 %1, i64 %2, i128 %3) {
; CHECK-LABEL: func3:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    sd s1, 8(sp) # 8-byte Folded Spill
; CHECK-NEXT:    .cfi_offset s1, -8
; CHECK-NEXT:    addi t1, t1, 1
; CHECK-NEXT:    addi t2, t2, 1
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vmv.v.x v8, a0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1, tu, ma
; CHECK-NEXT:    lui a0, %hi(.LCPI3_0)
; CHECK-NEXT:    flw ft0, %lo(.LCPI3_0)(a0)
; CHECK-NEXT:    vmv.s.x v8, s1
; CHECK-NEXT:    vsetivli zero, 0, e32, m1, ta, ma
; CHECK-NEXT:    vfmv.f.s ft1, v8
; CHECK-NEXT:    fadd.s ft0, ft1, ft0
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, tu, ma
; CHECK-NEXT:    vfmv.s.f v8, ft0
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, ma
; CHECK-NEXT:    vmv.x.s s1, v8
; CHECK-NEXT:    vslidedown.vi v8, v8, 1
; CHECK-NEXT:    vmv.x.s a0, v8
; CHECK-NEXT:    ld s1, 8(sp) # 8-byte Folded Reload
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %5 = add i64 %1, 1
  %6 = add i64 %2, 1

  %7 = bitcast i128 %3 to <4 x float>
  %8 = extractelement <4 x float> %7, i32 0
  %9 = fadd float %8, 1.0
  %10 = insertelement <4 x float> %7, float %9, i32 0
  %11 = bitcast <4 x float> %10 to i128

  %ret_val = insertvalue { ptr, i64, i64, i128 } undef, ptr %0, 0
  %ret_val2 = insertvalue { ptr, i64, i64, i128 } %ret_val, i64 %5, 1
  %ret_val3 = insertvalue { ptr, i64, i64, i128 } %ret_val2, i64 %6, 2
  %ret_val4 = insertvalue { ptr, i64, i64, i128 } %ret_val3, i128 %11, 3

  ret { ptr, i64, i64, i128 } %ret_val4
}
